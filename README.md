# Apache Flume customizations for developing a secure and universal database auditing platform

A highly scalable, secure and central repository that stores consolidated audit data and optionally listener, 
alert and OS log events generated by the database instances. This central platform will be used for reporting, 
alerting and security policy management. The reports will provide a holistic view of activity across all databases 
and will include compliance reports, activity reports and privilege reports. The alerting mechanism will 
detect and alert on abnormal activity, potential intrusion and much more. As audit data is vital record of 
activity, to protect this information the central repository will reside outside of existing databases and most 
likely in Hadoop eco-system.

## Developed components

Several implementations have been made for adapting Flume, both in the source and sink side.

* JDBCSource: a custom source which is able to collect data from database tables. It makes use of a ReliableJdbcEventReader which uses a JDBC driver for connecting to a database, so many types of databases are compatible. It provides a reliable way to read data from tables in order to avoid data loss and replicated events. This source produces JSONEvents (implements Flume Event interface), this events are deserialized as a JSON string.
* Interceptors that can modify events produced in the source:
    * DropNoJSONEventsInterceptor: drop all events which are not of the class JSONEvents.
    * JSONEventToCSVInterceptor: convert JSONEvents into normal Flume Events which body is a CSV with the values. Headers are copied. No JSONEvents are not touched.
    * DropDuplicatedEventsInterceptor: drop duplicated events. It only checks with the last "size" events. WARNING: this interceptor will drop events in case transaction to channel fails. In case the agent is restarted, hash for last events is lost so duplicates can appear.
* For some sinks, you may need to implement a custom parser for Flume Events:
    * JSONtoAvroParser: for Kite sink, this parser converts Flume Events which body is JSON into Avro records.
    * JSONtoElasticSearchEventSerializer: for Elasticsearch sink, this parser converts Flume Events which body is JSON into Elasticsearch XContentBuilder.
    
## Build

In order to build he project you will need to execute "bin/compile" script.

## Schema creation utility for Kite sink and JDBC source 

When using Kite as sink, previously you need to create a dataset. In order to create a Kite dataset, you should use the following command:

```
kite-dataset create <dataset_name> -s schema.avsc
```

This command receives an argument which points out a file (schema.avsc) containing the new dataset schema. You can infer the schema from a database table with an utility provided. An use case when this utility can be useful is when using ReliableJdbcAuditEventReader in the source.

Below command should be used for running this utility.

```
bin/infer-avro-schema-from-table -c <Connection URL> -t [<SCHEMA_NAME>.]<TABLE_NAME> -u <USERNSME> -p <PASSWORD> [-help]
 -c <CONNECTION_URL>       URL for connecting to database
 -t [<SCHEMA_NAME>.]<TABLE_NAME>           Table from which schema is inferred
 -u <USERNSME>             User to authenticate against database
 -p <PASSWORD>             User's password
 -dc <DRIVER_FQCN>         Fully qualified class name of JDBC driver (default: oracle.jdbc.driver.OracleDriver)
 -help                     Print help
```

This script adds to classpath all JAR files contained in lib folder, there should be placed the JDBC driver.

You can redirect the output of previous command to a local file and then, use this file for creating a Kite datasest. 

```
bin/infer-avro-schema-from-table -c jdbc:oracle:thin:@itrac13108.cern.ch:10121:IMT -u <user> -p <password> -t ADMIN_EMP > schema.avsc
kite-dataset create ADMIN_EMP_DATASET -s schema.avsc
```

## Configuration

### AuditSource

In order to use JDBCSource as source in your Flume agent, you need to specify the type of agent source as:

```
<agent_name>.sources.<source_name>.type = ch.cern.db.flume.source.JDBCSource 
```

NOTE: you must add the JDBC driver library to Flume's classpath when running Flume agent.

Find below all available configuration parameters:

```
<agent_name>.sources.<source_name>.batch.size = 100
<agent_name>.sources.<source_name>.batch.minimumTime = 10000
<agent_name>.sources.<source_name>.reader.committingFile = committed_value.backup
<agent_name>.sources.<source_name>.reader.committtedValue = NULL
<agent_name>.sources.<source_name>.reader.connectionDriver = oracle.jdbc.driver.OracleDriver
<agent_name>.sources.<source_name>.reader.connectionUrl = jdbc:oracle:oci:@
<agent_name>.sources.<source_name>.reader.username = sys as sysdba
<agent_name>.sources.<source_name>.reader.password = sys
<agent_name>.sources.<source_name>.reader.table = NULL
<agent_name>.sources.<source_name>.reader.table.columnToCommit = NULL
<agent_name>.sources.<source_name>.reader.table.columnToCommit.type = [TIMESTAMP (default)|NUMERIC|STRING]
<agent_name>.sources.<source_name>.reader.query = NULL
<agent_name>.sources.<source_name>.reader.query.path = NULL
<agent_name>.sources.<source_name>.duplicatedEventsProcessor = true
<agent_name>.sources.<source_name>.duplicatedEventsProcessor.size = 1000
<agent_name>.sources.<source_name>.duplicatedEventsProcessor.header = true
<agent_name>.sources.<source_name>.duplicatedEventsProcessor.body = true
<agent_name>.sources.<source_name>.duplicatedEventsProcessor.path = last_events.hash_list
```

Default values are written, parameters with NULL has not default value. Most configuration parameters do not require any further explanation. However, some of then are explained below.

".table", ".query" or ".query.path" parameter must be configured. ".columnToCommit" is always required.

Since it is a reliable reader, it requires one column of the table to be used for committing its value. Column to use for this purpose is configured with ".columnToCommit" parameter. Therefore, this column must be returned by the query. You would need to specify the type of this column in order to build the query properly. 

Last committed value is loaded when starting from ".committingFile" if specified. File is created if it does not exist. In case ".committingFile" does not exist or is empty, last committed value will be ".committedValue" if specified.

In case the query is not built properly or you want to use a custom one, you can use ".query" parameter (or ".query.path" for loading the query from a file). In that case ".table" and "columnToCommit.type" parameters are ignored. You should use the following syntax:

```
SELECT * FROM table_name [WHERE column_name >= ':committed_value'] ORDER BY column_name
```

NOTICE: default generated query makes use of ">=" for filtering new rows. Duplicated rows will be loaded from source table but they will be dropped by "duplicatedEventsProcessor" (default enabled). If "duplicatedEventsProcessor" is disabled and default generated query is used, duplicated events will be produced. You can configure your custom query in order to avoid duplicates when "duplicatedEventsProcessor" is not enabled. For more details of default behaviour please refer to: https://its.cern.ch/jira/browse/HRFAL-13

Some tips:
* ORDER BY clause is strongly recommended to be used since last value from "column to commit" will be used in further queries to get only last rows.
* If no value has been committed yet, part of the query between [] is removed. This is the case when we first start Flume and  ".committtedValue" is not configured.
* All occurrences of :committed_value will be replaced by last committed value.
* :committed_value should be between [], but if there will be always a committed value you do not need to place them.

Custom query example:

```
reader.query = SELECT * FROM UNIFIED_AUDIT_TRAIL [WHERE EVENT_TIMESTAMP > TIMESTAMP ':committed_value'] ORDER BY EVENT_TIMESTAMP
```

Query to be executed when a value has not been committed yet:

```
SELECT * FROM UNIFIED_AUDIT_TRAIL ORDER BY EVENT_TIMESTAMP
```

As soon as a value has been committed, query will be like:

```
SELECT * FROM UNIFIED_AUDIT_TRAIL WHERE EVENT_TIMESTAMP > TIMESTAMP '2013-11-08 12:11:31.123123 Europe/Zurich' ORDER BY EVENT_TIMESTAMP
```

### duplicatedEventsProcessor in JDBCSource

It compares new Events with last events. A set of last Event hashes is maintained, the size of this set can be configured with "size" parameter, default size is 1000.

Event's hash is calculate by default from headers (disabled if "header" parameter is set to false) and body (disabled if "body" parameter is set to false).

List of hashes is persisted into disk, it allows to maintain the list in case agent is restarted. File to persist hashes can be configured by "path" parameter.

### DropDuplicatedEventsInterceptor 

It compares new Events with last events. A set of last Event hashes is maintained, the size of this set can be configured with "size" parameter, default size is 1000.

Event's hash is calculate by default from headers (disabled if "header" parameter is set to false) and body (disabled if "body" parameter is set to false).

WARNINGS: this interceptor will drop not duplicated events in case transaction to channel fails. In case the agent is restarted, hash for last events is lost so duplicates could appear.

### Other components

They do not have any configuration parameters.








 

